# Expected value
E(xs) => sum([x * probability(x) for x in xs])

E(xs) is expected value (in general) among xs

# Surprise
P(x) => probability of x (from 0 to 1)

When P(x) is high, we expect the x
If the x occur and P(x) is really high we're not surprised
But if x occur and P(x) is really low then we're surprised
Which means that
The surpise is inverse of P

But it can't just be 1/P because in case P(x) = 1 we want surprise to be 0
And in case where P(x) -> 0 we want surprise -> inf

so

S(x) = log(1 / P(x))

# Entropy
Entropy is Expected value of Surprise <=> E(S(xs)) => E([log(1 / P(x) for x in xs])

Entropy is average surprise among xs

## What does it mean to take the Entropy of a string (char[])?

